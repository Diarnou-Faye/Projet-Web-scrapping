{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "17fc7fef-f5a9-4e7b-adce-110b99be157e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de la librairie \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dcc4268b-dd26-4220-b64d-5b765de35886",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_offres = []\n",
    "def nomNum_page(nomNum):\n",
    "    # Construction de l'url de la page\n",
    "    web_site_url = 'https://senjob.com/sn/offres-d-emploi.php' + nomNum\n",
    "\n",
    "    # recupération du contenu de la page HTML en utilisant la librairie requests\n",
    "    download = requests.get(web_site_url)\n",
    "\n",
    "    # Ensure that the reponse is valid\n",
    "    if download.status_code != 200:\n",
    "        print('Status code:', download.status_code)\n",
    "        raise Exception('Le téléchargement de la page web : ' + web_site_url + ' à échoué')\n",
    "\n",
    "    # Construct a beautiful soup document\n",
    "    page_content = BeautifulSoup(download.text)\n",
    "\n",
    "    return page_content\n",
    "\n",
    "def extract_data(nomNum):\n",
    "    pages_contents = nomNum_page(nomNum)\n",
    "    \n",
    "    #Enregistrons le contenu dans un fichier avec l'extension .html\n",
    "    #with open('Pages_senjobsn.html', 'w', encoding=\"utf-8\") as file:\n",
    "        #file.write(pages_contents)\n",
    "    \n",
    "    # Lions le contenu de notre fichier .html et créons un objet Beautiful Soup pour analyser le contenu\n",
    "    #with open('Pages_senjobsn.html','r', encoding=\"utf-8\") as f:\n",
    "        #pagesSources_html = f.read()\n",
    "        \n",
    "    #doc1_10 = BeautifulSoup(pagesSources_html,'html.parser')\n",
    "    \n",
    "    #Trouvons touts les span avec la classe offre title\n",
    "    matching_offre1_10 = pages_contents.find_all('span',class_='offre_title')\n",
    "    return matching_offre1_10\n",
    "\n",
    "def add_data (matching_offre1_10): \n",
    "    for offre1_10 in matching_offre1_10:\n",
    "        liste_offres.append(offre1_10.text.strip())\n",
    "        \n",
    "    return liste_offres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8be7e-8ba2-4342-87d3-463b34403f1a",
   "metadata": {},
   "source": [
    "#Extraire des informations du HTML à l'aide de Beautiful Soup\n",
    "\n",
    "Pour extraire des informations du code source HTML de notre page Web, nous allons utiliser la bibliothèque Beautiful Soup en l'important depuis le module **bs4**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e8af8-ed62-4353-8215-71dcbf4fd540",
   "metadata": {},
   "source": [
    "Ici nous allons Trouver et Stocker dans un tableau touts les Span corespondants aux offres avec la class='offre_title'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c76e0c-471f-45d3-b31d-d94d4bf49ca5",
   "metadata": {},
   "source": [
    "## Nous allons passer à l'Extraction des offres des 9 derniers page ensuite les ajouter dans notre tableau de liste d'offres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "45fc7845-79cc-4f9b-b4b0-b796d873c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction et ajout de la page 9 autres pages\n",
    "for i in range(1,10):\n",
    "    page = '?page='\n",
    "    #Concaténation de la variable page qui est une chaine et de i qui représente le numéro de la page et qui est un entier\n",
    "    s = \"%s%s\" % (page, i)\n",
    "    add_data(extract_data(s))\n",
    "    \n",
    "#add_data(extract_data('?page=3'))\n",
    "#add_data(extract_data('?page=4'))\n",
    "#add_data(extract_data('?page=5'))\n",
    "#add_data(extract_data('?page=6'))\n",
    "#add_data(extract_data('?page=7'))\n",
    "#add_data(extract_data('?page=8'))\n",
    "#add_data(extract_data('?page=9'))\n",
    "#add_data(extract_data('?page=10'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198aed2d-e313-4766-bb36-6e8e8558f6f8",
   "metadata": {},
   "source": [
    "# Liste de Touts les Offres de 9  dernières page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0c6a76cd-5bdc-466b-a529-f80a88bd4d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Informaticien (e)\n",
      "1 Chef Cuisinier\n",
      "2 Agent Service client\n",
      "3 CHARGE DE CLIENTELE\n",
      "4 Poste proposé : Commercial/le Terrain Digital Indépendant\n",
      "5 IP INGENIEUR\n",
      "6 RESPONSABLE APPROVISIONNEMENT\n",
      "7 Journaliste Web\n",
      "8 administrateur des ventes\n",
      "9 Conseiller(e) en Ressources Humaines\n",
      "10 Conseiller(e) en Ressources Humaines\n",
      "11 Responsable Programme – Bakel\n",
      "12 Responsable Programme – Bakel\n",
      "13 Chargé de marketing\n",
      "14 ASSISTANT ACHATS\n",
      "15 ADJOINT RESPONSABLE OPERATIONS COMMERCIALES\n",
      "16 CHARGE D'AFFAIRES\n",
      "17 CONSEILLER COMMERCIAL\n"
     ]
    }
   ],
   "source": [
    "for index, element in enumerate(liste_offres):\n",
    "    print(index, element)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
